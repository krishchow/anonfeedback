---
layout: post
title: "RE: Anonymous feedback responses"
date: Mon, 20 Feb 2017 12:01:54 EST
nav: post
category: CSC209
tags: [4649]
---

* content
{:toc}

[quote]I honestly don't agree with the concept of using auto markers. I understand of course. It's really tedious and sometimes impossible to have to grade hundreds of assignments with crazy amounts of code. Still, I don't really agree with the auto marker system. To me, if a student understands 50% of a concept, they should get a 50%. If they understand 10%, they get a 10%. If they understand it all, they ace it. But with auto markers, it seems that even a student with 50% understanding will get a 0% or close to such. I may not understand how the auto markers work, but they don't seem AS accurate of an assessment to me as a human assessment would be. Hopefully I'm not coming off as angry, I'm honestly just trying to give my opinion on the system. Sorry if I come off as rude. [/quote]
<!-- more -->
<p>\n\nNo worries. You're not being rude at all.\n\nI think I'm hearing two concerns here: one about how automarkers evaluate code and one about how we assign marks. \n\n[quote] To me, if a student understands 50% of a concept, they should get a 50%. If they understand 10%, they get a 10%. If they understand it all, they ace it. But with auto markers, it seems that even a student with 50% understanding will get a 0% or close to such.[/quote]\n\nI don't think this is entirely an issue with automarkers: it's more an issue with how I set the marking scheme. That's related to personal beliefs about understanding and measuring understanding. Bear with me. This will take a few minutes ...\n\nSo, an automarker runs a series of tests and then assigns a mark based on an encoded marking scheme. The tests it runs are necessarily not comprehensive: we won't think to test everything. Also, the marking scheme suffers from the limitations of its developer. For example, if I don't think that anyone will make mistake X, then either (a) it won't be in the tests and hence, won't be evaluated or (b) all of the tests will require that functionality, so it'll cause students to get a zero. The latter is why we have resubmissions (and why we sometimes employ TAs to spot-check assignments and make sure the marking seems fair).  \n\nThere are also things we can't evaluate (easily ... or at all) using autotests. For example, I can't easily evaluate whether you're using a good design or not, and it's difficult for me to evaluate your coding style. There is some neat work in machine learning tackling this problem, by the way. I saw some work a few years ago that demonstrated that a properly trained expert system could grade essays more accurately than human graders. But that system is still a bit of a research tool.\n\nOn the other hand, a human grader will exercise some autonomy and will display flexibility. If they see a problem that wasn't covered in the tests, they can deduct for it. If they think the tests are being too harsh, they can provide part marks where an automarker would assign a 0. They can also apply complex criteria to evaluate somewhat subjective things, like code style. \n\nAs a result, human graders often provide higher marks than the grading scheme prescribes. I've seen one dramatic example in my own research. A few years ago, I started using a sequence of small, focused short answer questions ("Fill in the condition for this if statement that ...") in place of some larger programming problems ("Write a function that ..."). Grades [i]plummeted[/i]. Why? Because when the TAs were asked to mark a series of small things, they assigned each small question a 0, .5, or 1 -- and students rarely got it entirely correct. On the other hand, when they graded a large programming problem, they could assign any mark between 0 and 4, and they [i]actively looked for any reason to give part marks[/i]. So where a student might easily get a 0 or a 1 on a sequence of 4 1-point  questions, they would have gotten 1.5 or 2 on a larger question worth 4 points.  \n\nSo, which result is correct? Did the student deserve a 0 because they couldn't answer 4 tightly-focused questions? Or a 2 because they could write some code that looked kind of correct, even if it didn't actually answer the question at all? Now this becomes related again ...\n\n[quote] To me, if a student understands 50% of a concept, they should get a 50%. If they understand 10%, they get a 10%. If they understand it all, they ace it.[/quote]\n\nI have no idea how to actually evaluate understanding. I can't unscrew the back of your head, pour out what you "know," and measure it. As a result, 10% and 30% are, to me, indistinguishable. They're both low enough that if I were to ask you a series of questions, I'd probably conclude, "Nope, they don't get it." In the end, since I can't evaluate understanding, I have to evaluate outcomes: can you [i]do[/i] some task. If you can, then I conclude that you understand something. If you can't, then I have to conclude that you ... don't, or at least, not enough. If I ask a sequence of questions, I can start to approximate how well you understand something: if I ask 10 questions and you can answer 5 of them, then maybe you "half get it". \n\nThere's a related issue: consistency. One TA might assign the student a 2 on the large programming question, while another would assign a 1, and a third would assign a 2.5.  This is a huge problem, and it's difficult to fix. There have been times in the past where [i]every midterm I spot-check after a grading session has had an error in marking in it[/i]. Those errors in marking go both ways: points deducted when they shouldn't be, and points not deducted when they should. \n\nYour answers probably differ from mine, but here's my take on the questions raised in the last few paragraphs:\n\n1) [i]Should we be marking holistically or using focused, specific questions?[/i] In the context in which we were using the smaller, focused questions -- a first year programming course, where the desired outcome was the ability to use conditionals and loops -- I think they do a better job evaluating understanding than the more complex, integrative question. In a later year course, where the focus is on the design or the integration of components, I think I'd prefer a more holistic evaluation. But in the context of the example: I think the lower mark was more correct.\n\n2) [i]How big a deal is consistency?[/i] In a large class, consistency is very important. Within reason, I will exchange flexibility and even accuracy for consistency. That is: if I'm teaching 200+ students, I'd rather be "fair" than "correct" (to a point).\n\nAnd that, in ... a long screed, is why I use automarkers. The strength of an automarker is its consistency, and with a well-designed series of tests, you can try to get at specific pieces of functionality, rather than holistically evaluating the entire assignment. That doesn't mean that automarkers are all good. It's very easy to make a mistake with an automarker or to not factor in an important criteria into the marking scheme. That's why we have a a resubmission period and why we also have the TAs spot check the marking.\n\nThere are also things that the automarker can't mark, and as a result, the TAs will be heavily involved in the marking for A2. They'll be provided with the results of the automarker (and a rough assigned mark), and they'll be asked to spot check those results and also to fill in additional marks related to how the program was designed and how pointers were used.</p>
